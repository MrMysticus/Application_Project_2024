{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file for making the predictions\n",
    "\n",
    "- all functions in one py file??\n",
    "\n",
    "- mit den daten die predictions machen\n",
    "- temporary predictions data file (jede stunde werden neue predictions generiert)\n",
    "- model separat speichern (damit man es ändern kann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta, timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = 24\n",
    "in_channels = 1\n",
    "out_channels = 2\n",
    "kernel_size = 4\n",
    "stride = 2\n",
    "dropout_prob = 0.2\n",
    "prediction_length_steps = 5\n",
    "activation = torch.nn.ReLU()\n",
    "\n",
    "original_feature_count = 1 # full_dataset.shape[1]\n",
    "target_feature_index = 0\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, out_channels, kernel_size, stride, dropout_prob):\n",
    "        super(ConvModel, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size # size of features # sequence length, not feature count\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.conv1d_output_size=out_channels*math.floor((input_size-kernel_size)/stride +1)\n",
    "        \n",
    "        self.hidden_layer_size=int(self.conv1d_output_size/2)\n",
    "        self.lin = nn.Linear(self.conv1d_output_size, self.hidden_layer_size)  \n",
    "        self.lin2 = nn.Linear(self.hidden_layer_size, prediction_length_steps)  \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_conv_output = activation(self.conv1d(x))\n",
    "        x_reshape = x_conv_output.reshape(x_conv_output.size(0), -1)\n",
    "        x_lin1 = activation(self.lin(x_reshape))\n",
    "        x_lin1 = self.dropout(x_lin1)\n",
    "        return self.lin2(x_lin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions\n",
    "\n",
    "def inverse_scale_target(scaler, scaled_target, target_feature_index, original_feature_count):\n",
    "    # Prepare a dummy matrix with zeros\n",
    "    dummy = np.zeros((scaled_target.shape[0], original_feature_count))\n",
    "\n",
    "    # Place scaled target feature where it originally belonged in full dataset\n",
    "    dummy[:, target_feature_index] = scaled_target.flatten()\n",
    "\n",
    "    # Use inverse_transform, which applies only to non-zero entries when split like this\n",
    "    inversed_full = scaler.inverse_transform(dummy)\n",
    "\n",
    "    # Extract only the inversely transformed target value\n",
    "    return inversed_full[:, target_feature_index]\n",
    "\n",
    "\n",
    "# Return the prediction\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return model(data)\n",
    "\n",
    "\n",
    "def update_and_save_predictions(DATA_FILENAME, MODEL_FILENAME, SCALER_FILENAME, PREDICTIONS_FILENAME):\n",
    "    # make überprüfung, ob predictions needed at this time? otherwise the predictions would be generated every time the application gets refreshed\n",
    "    try:\n",
    "        # load in data_temp\n",
    "        # Laden des existierenden DataFrame\n",
    "        data_temp = pd.read_csv(DATA_FILENAME)\n",
    "        data_temp['time_utc'] = pd.to_datetime(data_temp['time_utc'])\n",
    "        latest_data_time = data_temp['time_utc'].max()\n",
    "    except Exception as e:\n",
    "        print(f'No {DATA_FILENAME} file found.')\n",
    "        print(f'Error: {e}')\n",
    "        \n",
    "    # Prüfen, ob predictions.csv vorhanden ist\n",
    "    if os.path.exists(PREDICTIONS_FILENAME):\n",
    "        # Laden des existierenden DataFrame\n",
    "        data_temp_predictions = pd.read_csv(PREDICTIONS_FILENAME)\n",
    "        data_temp_predictions['prediction_time_utc'] = pd.to_datetime(data_temp_predictions['prediction_time_utc'])\n",
    "        earliest_prediction_time = data_temp_predictions['prediction_time_utc'].min()\n",
    "        # überprüfen ob neue predictions necessary\n",
    "        if earliest_prediction_time > latest_data_time:\n",
    "            print(\"No new predictions necessary, predictions are up to date.\")\n",
    "            print('-------------')\n",
    "            print(f'Time in UTC:\\nEarliest Prediction for: {earliest_prediction_time}\\nLatest Data for: {latest_data_time}')\n",
    "            return  # Beenden der Funktion, wenn keine neuen Predictions nötig sind\n",
    "        else:\n",
    "            # Altes Daten löschen, da neue Predictions notwendig sind\n",
    "            data_temp_predictions = pd.DataFrame(columns=['entityId', 'prediction_time_utc', 'prediction_availableBikeNumber'])\n",
    "\n",
    "    else:\n",
    "        # Erstellen eines leeren DataFrame, wenn die Datei nicht existiert\n",
    "        data_temp_predictions = pd.DataFrame(columns=['entityId', 'prediction_time_utc', 'prediction_availableBikeNumber']) # to be adjusted\n",
    "\n",
    "    try:\n",
    "            # model saved torch.save(cnn_model.state_dict(), 'cnn_model.pth')\n",
    "        # load in the model\n",
    "        # Modellinitialisierung (Stellen Sie sicher, dass Sie alle benötigten Hyperparameter angeben)\n",
    "        loaded_model = ConvModel(input_size, out_channels, kernel_size, stride, dropout_prob)\n",
    "        # Laden der Modellparameter\n",
    "        loaded_model.load_state_dict(torch.load(MODEL_FILENAME, weights_only=True))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'No {MODEL_FILENAME} file found.')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "    try:\n",
    "            # scalar saved joblib.dump(scaler, 'scaler.pkl')\n",
    "        # load in the scalar\n",
    "        scaler = joblib.load(SCALER_FILENAME)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'No {SCALER_FILENAME} file found.')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataframes = []\n",
    "        # for every unique entity id make predictions\n",
    "        entityId_list = data_temp.entityId.unique()\n",
    "        for entity in entityId_list:\n",
    "            data_for_prediction = data_temp[data_temp['entityId'] == entity]\n",
    "\n",
    "            # make the data in such form for model to use\n",
    "            # Select the 'availableBikeNumber' column, convert to float and create a tensor\n",
    "            data_for_prediction = torch.tensor(data_for_prediction['availableBikeNumber'].values).float()\n",
    "            data_for_prediction = data_for_prediction.unsqueeze(0).unsqueeze(0)  # Das Ergebnis ist ebenfalls [1, 1, 24]\n",
    "\n",
    "            # make predictions\n",
    "            entityId_predictions = predict(loaded_model, data_for_prediction)\n",
    "            entityId_predictions = entityId_predictions.unsqueeze(-1)\n",
    "\n",
    "            # make predictions real numbers, if model used scaled data for prediction\n",
    "            num_samples, prediction_length, _ = entityId_predictions.shape\n",
    "            entityId_predictions_reshaped = entityId_predictions.reshape(num_samples * prediction_length, -1)\n",
    "            # Inverse transform for target feature predictions\n",
    "            entityId_predictions_bikes = inverse_scale_target(scaler, entityId_predictions_reshaped, target_feature_index, original_feature_count).reshape(num_samples, prediction_length, -1)\n",
    "\n",
    "            # append to dataframe with entityId and predictions\n",
    "            # Assign dates to each prediction\n",
    "            start_date = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0) \n",
    "            # Erzeugen einer Liste von Zeitstempeln für jede Vorhersage\n",
    "            date_list = [start_date + timedelta(hours=i) for i in range(prediction_length)]\n",
    "            \n",
    "            # Create DataFrame for current entity predictions\n",
    "            temp_df = pd.DataFrame({\n",
    "                'entityId': entity,\n",
    "                'prediction_time_utc': date_list,\n",
    "                'prediction_availableBikeNumber': entityId_predictions_bikes.squeeze().tolist()\n",
    "            })\n",
    "\n",
    "            # Hinzufügen des temporären DataFrame zur Liste\n",
    "            dataframes.append(temp_df)\n",
    "\n",
    "        # Zusammenführen aller temporären DataFrames zu einem finalen DataFrame\n",
    "        data_temp_predictions = pd.concat(dataframes, ignore_index=True)\n",
    "        # save them in predictions.csv\n",
    "        data_temp_predictions.to_csv(PREDICTIONS_FILENAME, index=False)\n",
    "        earliest_prediction_time = data_temp_predictions['prediction_time_utc'].min()\n",
    "\n",
    "        print(f'Predictions made successfully and saved for STATION_IDS:{entityId_list}')\n",
    "        print('-------------')\n",
    "        print(f'Time in UTC:\\nEarliest Prediction for: {earliest_prediction_time}\\nLatest Data for: {latest_data_time}')\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error in function.')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "    return entityId_predictions_bikes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konfiguration\n",
    "\n",
    "DATA_FILENAME = 'data_temp.csv'\n",
    "MODEL_FILENAME = 'cnn_model.pth'\n",
    "SCALER_FILENAME = 'scaler.pkl'\n",
    "PREDICTIONS_FILENAME = 'predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions made successfully and saved for STATION_IDS:[24367 24370 24397 24399]\n",
      "-------------\n",
      "Time in UTC:\n",
      "Earliest Prediction for: 2024-11-26 07:00:00+00:00\n",
      "Latest Data for: 2024-11-26 06:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "### Usage\n",
    "\n",
    "entityId_predictions_bikes = update_and_save_predictions(DATA_FILENAME, MODEL_FILENAME, SCALER_FILENAME, PREDICTIONS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityId_predictions_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
