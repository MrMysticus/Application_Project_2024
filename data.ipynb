{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file for creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt !\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import dotenv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_access_token(USERNAME_EMAIL, PASSWORD, CLIENT_SECRET, dotenv_path='.env'):\n",
    "    \"\"\"\n",
    "    Requests an access token using user credentials and client information from an authorization server.\n",
    "\n",
    "    This function posts user credentials and client details to the specified token URL of an OAuth authentication server,\n",
    "    and tries to retrieve an access token. If successful, the access token is saved to the specified dotenv file and returned.\n",
    "\n",
    "    :param USERNAME_EMAIL: The username or email associated with the user account.\n",
    "    :type USERNAME_EMAIL: str\n",
    "    :param PASSWORD: The password for the user account.\n",
    "    :type PASSWORD: str\n",
    "    :param CLIENT_SECRET: The secret key associated with the client application.\n",
    "    :type CLIENT_SECRET: str\n",
    "    :param dotenv_path: Path to the dotenv file where the obtained access token will be saved, default is '.env'.\n",
    "    :type dotenv_path: str\n",
    "\n",
    "    :return: The access token if successfully requested, otherwise None.\n",
    "    :rtype: str or None\n",
    "    \"\"\"\n",
    "\n",
    "    token_url = 'https://accounts.kielregion.addix.io/realms/infoportal/protocol/openid-connect/token'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'grant_type': 'password',\n",
    "        'username': USERNAME_EMAIL, \n",
    "        'password': PASSWORD,\n",
    "        'client_id': 'quantumleap',\n",
    "        'client_secret': CLIENT_SECRET\n",
    "    }\n",
    "\n",
    "    response = requests.post(token_url, headers=headers, data=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        access_token = token_data.get('access_token')\n",
    "        print(\"Bearer Token successfully requested\")\n",
    "        if access_token:\n",
    "            dotenv.set_key(dotenv_path, 'ACCESS_TOKEN', access_token)\n",
    "            print(\"Access Token successfully written to the .env file.\")\n",
    "            return access_token\n",
    "        else:\n",
    "            print(\"Access token is not available in the response.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer Token successfully requested\n",
      "Access Token successfully written to the .env file.\n"
     ]
    }
   ],
   "source": [
    "# .env\n",
    "config = dotenv.dotenv_values('.env')\n",
    "\n",
    "PASSWORD = config['PASSWORD']\n",
    "\n",
    "CLIENT_SECRET = config['CLIENT_SECRET']\n",
    "\n",
    "USERNAME_EMAIL = config['USERNAME_EMAIL']\n",
    "\n",
    "# Example usage:\n",
    "ACCESS_TOKEN = request_access_token(USERNAME_EMAIL, PASSWORD, CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions\n",
    "\n",
    "def fetch_station_data(station_id, from_date, to_date, BASE_URL, ACCESS_TOKEN):\n",
    "    \"\"\"\n",
    "    Retrieves data for a specified bike hire docking station over a given time period.\n",
    "\n",
    "    This function connects to a specified URL to access data regarding the availability of bikes. It\n",
    "    makes an HTTP GET request with authentication and specific query parameters to gather\n",
    "    aggregated data across the specified dates.\n",
    "\n",
    "    :param station_id: Unique identifier for the bike hire docking station.\n",
    "    :type station_id: str\n",
    "    :param from_date: The start date and time from when the data is to be fetched.\n",
    "    :type from_date: datetime\n",
    "    :param to_date: The end date and time until when the data is to be fetched.\n",
    "    :type to_date: datetime\n",
    "    :param BASE_URL: The base URL of the API where the bike data is hosted.\n",
    "    :type BASE_URL: str\n",
    "    :param ACCESS_TOKEN: The access token for authenticating the API request.\n",
    "    :type ACCESS_TOKEN: str\n",
    "\n",
    "    :return: A JSON object containing the response data if successful, None otherwise. The JSON includes\n",
    "             aggregated available bike numbers per hour. If the request fails, an error message and status\n",
    "             code are printed.\n",
    "    :rtype: dict or None\n",
    "    \n",
    "    :raises Exception: Raises an error with status code and text if the response is unsuccessful.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{BASE_URL}{station_id}\"\n",
    "    headers = {\n",
    "        'NGSILD-Tenant': 'infoportal',\n",
    "        'Authorization': f'Bearer {ACCESS_TOKEN}'\n",
    "    }\n",
    "    params = {\n",
    "        'type': 'BikeHireDockingStation',\n",
    "        'fromDate': from_date.isoformat(),\n",
    "        'toDate': to_date.isoformat(),\n",
    "        'attrs': 'availableBikeNumber',\n",
    "        'aggrPeriod': 'hour',\n",
    "        'aggrMethod': 'avg'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f'Got a response for station_id: {station_id}')\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def create_dataframe_from_api_data(data):\n",
    "    \"\"\"\n",
    "    Converts data received from an API response into a structured pandas DataFrame.\n",
    "\n",
    "    This function parses data from a JSON-like dictionary that includes time indices,\n",
    "    entity identifiers, and various attributes into a DataFrame that can be used for \n",
    "    further data analysis or visualization.\n",
    "\n",
    "    :param data: The data received from an API request, expected to include keys like \n",
    "                 'index', 'entityId', and 'attributes' which contain measurement values.\n",
    "    :type data: dict\n",
    "\n",
    "    :return: A DataFrame with time indices, an entity id extracted from 'entityId', and columns \n",
    "             for each attribute contained within 'attributes'. This DataFrame is restructured to \n",
    "             place 'entityId' and 'time_utc' columns first.\n",
    "    :rtype: pandas.DataFrame\n",
    "\n",
    "    :raises ValueError: If essential keys such as 'index', 'entityId', or 'attributes' are missing in the data.\n",
    "    \"\"\"\n",
    "\n",
    "    if not all(key in data for key in ['index', 'entityId', 'attributes']):\n",
    "        raise ValueError(\"Data missing one of the essential keys: 'index', 'entityId', 'attributes'\")\n",
    "    \n",
    "    # Dictionary to store attribute values\n",
    "    # attribute_data = {}\n",
    "\n",
    "    # Extract the index from the response\n",
    "    time_index = pd.to_datetime(data['index'])\n",
    "\n",
    "    # Extract entityId and entityType\n",
    "    entity_id = data['entityId']\n",
    "\n",
    "    # Extract the number after \"KielRegion\" from the entityId\n",
    "    match = re.search(r'KielRegion:(\\d+)', entity_id)\n",
    "    entity_id_number = match.group(1) if match else ''  # Get the number or set to empty if not found\n",
    "\n",
    "    # Loop through each attribute dictionary in 'attributes'\n",
    "    # for attribute in data['attributes']:\n",
    "    #     attr_name = attribute['attrName']\n",
    "    #     attribute_data[attr_name] = attribute.get('values', [])\n",
    "\n",
    "    # Dictionary to accumulate attribute values\n",
    "    attribute_data = {attr['attrName']: attr.get('values', []) for attr in data['attributes']}\n",
    "    \n",
    "    # Create a pandas DataFrame from the dictionary\n",
    "    df = pd.DataFrame(attribute_data)\n",
    "    # Add the entityId number and index values as new columns\n",
    "    df['entityId'] = entity_id_number\n",
    "    df['time_utc'] = time_index\n",
    "\n",
    "    # Reorder the columns to have 'entityId' first, then 'time', followed by the rest\n",
    "    column_order = ['entityId', 'time_utc'] + [col for col in df.columns if col not in ['entityId', 'time_utc']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def update_and_save_station_data(DATA_FILENAME, STATIONS_FILENAME, START_DATE, END_DATE, BASE_URL, ACCESS_TOKEN):\n",
    "    \"\"\"\n",
    "    Updates and saves bike station data by fetching new data for specified stations and dates, then combining it with existing data.\n",
    "\n",
    "    This function first checks if there exists previous data in a specified CSV file,\n",
    "    and loads it if available. It then identifies any gaps in the data between START_DATE and END_DATE,\n",
    "    and makes API requests to fetch missing data for those specific time periods and station IDs.\n",
    "    The newly fetched data is then combined with the previously existing data, the combined data is sorted and saved back to the CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - DATA_FILENAME (str): The file path for reading and writing data.\n",
    "    - STATIONS_FILENAME (str): The file path for reading and writing station data.\n",
    "    - START_DATE (datetime): The starting datetime from which data needs to be fetched.\n",
    "    - END_DATE (datetime): The ending datetime until which data needs to be fetched.\n",
    "    - BASE_URL (str): The base URL to which API requests should be made.\n",
    "    - ACCESS_TOKEN (str): The token used for authenticating API requests.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value but prints a message to the console about the process completion or any errors encountered.\n",
    "\n",
    "    No return value is expected. The function logs messages indicating successful or failed data fetching attempts,\n",
    "    and shows the number of new records fetched and total unique stations updated.\n",
    "    If no new data is fetched, it notifies that existing data is used.\n",
    "\n",
    "    Side Effects:\n",
    "    - Read and write operations on a CSV file.\n",
    "    - API requests sent to a remote service.\n",
    "    - Potentially modifies global state if global variables or mutable data types are passed and manipulated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prüfen, ob data_temp.csv vorhanden ist\n",
    "    if os.path.exists(DATA_FILENAME):\n",
    "        # Laden des existierenden DataFrame\n",
    "        old_data_temp = pd.read_csv(DATA_FILENAME)\n",
    "        # make 'time_utc' in datetime\n",
    "        old_data_temp['time_utc'] = pd.to_datetime(old_data_temp['time_utc'])\n",
    "        # lösche alle daten vor START_DATE\n",
    "        old_data_temp = old_data_temp[old_data_temp['time_utc'] >= START_DATE]\n",
    "    else:\n",
    "        # Erstellen eines leeren DataFrame, wenn die Datei nicht existiert\n",
    "        old_data_temp = pd.DataFrame(columns=['entityId', 'time_utc'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # prüfen ob station_data.csv vorhanden\n",
    "    try:\n",
    "        # Laden des existierenden DataFrame\n",
    "        stations_data = pd.read_csv(STATIONS_FILENAME)\n",
    "        # make entity id list\n",
    "        STATION_IDS = stations_data['entityId'].tolist()\n",
    "    except Exception as e:\n",
    "        print(f'No {STATIONS_FILENAME} file exists. Please provide such file with these columns:\\nentityId, station_name, maximum_capacity, longitude, latitude, subarea')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "\n",
    "\n",
    "    # - timedelta(hours=1), damit der request_start_date nicht gleich END_DATE ist\n",
    "    # full_date_range = all timestamps (until now) of the timewindow needed for the model for prediction\n",
    "    full_date_range = pd.date_range(start=START_DATE, end=END_DATE - timedelta(hours=1), freq='h') \n",
    "    # Liste von DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # entfernen der einträge der station ids, die nicht in STATION_IDS ist\n",
    "    # Maske, die True ist für jede Zeile, deren entityId NICHT in STATION_IDS ist\n",
    "    mask = ~old_data_temp['entityId'].isin(STATION_IDS)\n",
    "    # Entfernen dieser Zeilen\n",
    "    old_data_temp = old_data_temp[~mask]\n",
    "\n",
    "    for station_id in STATION_IDS:\n",
    "        # überprüfe für station_id, ob der zeitraum von START_DATE bis END_DATE in old_data_temp vorhanden ist:\n",
    "        # select one station\n",
    "        station_data = old_data_temp[old_data_temp['entityId'] == station_id]\n",
    "        # extract available dates\n",
    "        available_dates = station_data['time_utc']\n",
    "        # Ermitteln der fehlenden Daten\n",
    "        missing_dates = full_date_range[~full_date_range.isin(available_dates)]\n",
    "\n",
    "        # wenn ja, skip diese station_id\n",
    "        # wenn nein, mache ein request_start_date\n",
    "\n",
    "        # Daten nur für fehlende Zeiten anfordern\n",
    "        if not missing_dates.empty:\n",
    "            request_start_date = missing_dates[0]\n",
    "            # und requeste die nicht vorhandenen stunden bis zum END_DATE\n",
    "            data = fetch_station_data(station_id, request_start_date, END_DATE, BASE_URL, ACCESS_TOKEN)\n",
    "            if data:\n",
    "                df = create_dataframe_from_api_data(data)\n",
    "                # und appende sie an das dataframe\n",
    "                dataframes.append(df)\n",
    "\n",
    "    if dataframes:\n",
    "        # Alle neuen DataFrames der Stationen zusammenführen\n",
    "        new_data_temp = pd.concat(dataframes)\n",
    "        # make the entitiy_id a number \n",
    "        new_data_temp['entityId'] = new_data_temp['entityId'].astype('int64')\n",
    "        # Zusammenführen des alten DataFrames mit dem neuen\n",
    "        combined_data_temp = pd.concat([old_data_temp, new_data_temp])\n",
    "        # Sortieren, nach entitiyId und time_utc\n",
    "        combined_data_temp = combined_data_temp.sort_values(by=['entityId', 'time_utc'])\n",
    "        # resete index\n",
    "        combined_data_temp = combined_data_temp.reset_index(drop=True)\n",
    "        # DataFrame in eine CSV-Datei speichern\n",
    "        combined_data_temp.to_csv(DATA_FILENAME, index=False)\n",
    "\n",
    "        # count new records and unique Ids \n",
    "        total_new_records = len(new_data_temp)\n",
    "        unique_stations = new_data_temp['entityId'].nunique()\n",
    "\n",
    "        print(f'{total_new_records} new records fetched for {unique_stations} stations.')\n",
    "        print(f'Data successfully loaded and saved for STATION_IDS:{STATION_IDS}')\n",
    "    else:\n",
    "        print('No new data to process, data for every station is available. Existing data used.')\n",
    "\n",
    "    print('-------------')\n",
    "    print(f'Time in UTC:\\nStart Date: {START_DATE}\\nEnd Date: {END_DATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konfigurations\n",
    "\n",
    "# .env file anpassen, für application mit password, username email for access token\n",
    "# config = dotenv.dotenv_values(\".env\")\n",
    "\n",
    "# ACCESS_TOKEN = config[\"ACCESS_TOKEN\"]\n",
    "\n",
    "BASE_URL = \"https://apis.kielregion.addix.io/ql/v2/entities/urn:ngsi-ld:BikeHireDockingStation:KielRegion:\"\n",
    "\n",
    "# API mit UTC time steps\n",
    "# Calculate the end date by rounding down to the closest whole hour in UTC !,\n",
    "# to make sure to get hourly averages for whole hours with API request\n",
    "END_DATE = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "START_DATE = END_DATE - timedelta(days=1) # timedelta anpassen an model sliding window length (=24 hours)\n",
    "\n",
    "DATA_FILENAME = 'data_temp.csv'\n",
    "STATIONS_FILENAME = 'stations.csv' # station ids from file einlesen, um änderungen zu haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data to process, data for every station is available. Existing data used.\n",
      "-------------\n",
      "Time in UTC:\n",
      "Start Date: 2024-11-25 10:00:00+00:00\n",
      "End Date: 2024-11-26 10:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "### Usage\n",
    "\n",
    "update_and_save_station_data(DATA_FILENAME, STATIONS_FILENAME, START_DATE, END_DATE, BASE_URL, ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIONS_FILENAME = 'stations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'entityId': [24370, 24397, 24367],\n",
    "    'station_name': ['StationA', 'StationB', 'StationC'],\n",
    "    'maximum_capacity': [100, 200, 150],\n",
    "    'longitude': [8.6821, 6.9603, 9.1829],\n",
    "    'latitude': [50.1109, 50.9375, 48.7758],\n",
    "    'subarea': ['North', 'South', 'West']\n",
    "}\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entityId</th>\n",
       "      <th>station_name</th>\n",
       "      <th>maximum_capacity</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>subarea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24370</td>\n",
       "      <td>StationA</td>\n",
       "      <td>100</td>\n",
       "      <td>8.6821</td>\n",
       "      <td>50.1109</td>\n",
       "      <td>North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24397</td>\n",
       "      <td>StationB</td>\n",
       "      <td>200</td>\n",
       "      <td>6.9603</td>\n",
       "      <td>50.9375</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24367</td>\n",
       "      <td>StationC</td>\n",
       "      <td>150</td>\n",
       "      <td>9.1829</td>\n",
       "      <td>48.7758</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entityId station_name  maximum_capacity  longitude  latitude subarea\n",
       "0     24370     StationA               100     8.6821   50.1109   North\n",
       "1     24397     StationB               200     6.9603   50.9375   South\n",
       "2     24367     StationC               150     9.1829   48.7758    West"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(STATIONS_FILENAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
