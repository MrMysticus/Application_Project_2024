{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file for creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt !\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import dotenv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_access_token(USERNAME_EMAIL, PASSWORD, CLIENT_SECRET, dotenv_path='.env'):\n",
    "    \"\"\"\n",
    "    Requests an access token using user credentials and client information from an authorization server.\n",
    "\n",
    "    This function posts user credentials and client details to the specified token URL of an OAuth authentication server,\n",
    "    and tries to retrieve an access token. If successful, the access token is saved to the specified dotenv file and returned.\n",
    "\n",
    "    :param USERNAME_EMAIL: The username or email associated with the user account.\n",
    "    :type USERNAME_EMAIL: str\n",
    "    :param PASSWORD: The password for the user account.\n",
    "    :type PASSWORD: str\n",
    "    :param CLIENT_SECRET: The secret key associated with the client application.\n",
    "    :type CLIENT_SECRET: str\n",
    "    :param dotenv_path: Path to the dotenv file where the obtained access token will be saved, default is '.env'.\n",
    "    :type dotenv_path: str\n",
    "\n",
    "    :return: The access token if successfully requested, otherwise None.\n",
    "    :rtype: str or None\n",
    "    \"\"\"\n",
    "\n",
    "    token_url = 'https://accounts.kielregion.addix.io/realms/infoportal/protocol/openid-connect/token'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'grant_type': 'password',\n",
    "        'username': USERNAME_EMAIL, \n",
    "        'password': PASSWORD,\n",
    "        'client_id': 'quantumleap',\n",
    "        'client_secret': CLIENT_SECRET\n",
    "    }\n",
    "\n",
    "    response = requests.post(token_url, headers=headers, data=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        access_token = token_data.get('access_token')\n",
    "        print(\"Bearer Token successfully requested\")\n",
    "        if access_token:\n",
    "            dotenv.set_key(dotenv_path, 'ACCESS_TOKEN', access_token)\n",
    "            print(\"Access Token successfully written to the .env file.\")\n",
    "            return access_token\n",
    "        else:\n",
    "            print(\"Access token is not available in the response.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer Token successfully requested\n",
      "Access Token successfully written to the .env file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJPbWpOY21GRmgwTjV6Wlg1eHg5Zk9mbE0xNFljTmY3WFlieG02OTJtczhBIn0.eyJleHAiOjE3MzI2OTIxMTcsImlhdCI6MTczMjU0ODExNywianRpIjoiOWVjOGY4MzctMTM4Zi00Mjc2LThhYWMtOGQ0NjUxNjZhN2M0IiwiaXNzIjoiaHR0cHM6Ly9hY2NvdW50cy5raWVscmVnaW9uLmFkZGl4LmlvL3JlYWxtcy9pbmZvcG9ydGFsIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6ImNkMjRhNmMxLWE4MTYtNDkyYy1iZmJlLTU4ZDFjMTQ5ZTA2OSIsInR5cCI6IkJlYXJlciIsImF6cCI6InF1YW50dW1sZWFwIiwic2Vzc2lvbl9zdGF0ZSI6ImQ5MTdlMTA4LWU5MzQtNGJlMS1iMDhiLTEwM2JlZTdmNmUyNSIsImFjciI6IjEiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiZGVmYXVsdC1yb2xlcy1pbmZvcG9ydGFsIiwib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7InF1YW50dW1sZWFwIjp7InJvbGVzIjpbImRhdGEtY29uc3VtZXItcWwiXX0sImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoicm9sZXMgcHJvZmlsZSBlbWFpbCIsInNpZCI6ImQ5MTdlMTA4LWU5MzQtNGJlMS1iMDhiLTEwM2JlZTdmNmUyNSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJuYW1lIjoiSnVzdHVzIEhlaWxpbmdicnVubmVyIiwicHJlZmVycmVkX3VzZXJuYW1lIjoianVzdHVzLmhlaWxpbmdicnVubmVyQHN0dWRlbnQuZmgta2llbC5kZSIsImdpdmVuX25hbWUiOiJKdXN0dXMiLCJmYW1pbHlfbmFtZSI6IkhlaWxpbmdicnVubmVyIiwiZW1haWwiOiJqdXN0dXMuaGVpbGluZ2JydW5uZXJAc3R1ZGVudC5maC1raWVsLmRlIn0.VTfp_Idox7OerBnBTHYrqYGVQHnx4fGeaWgv29pu2DgqubPkCE6uxCmPrhglT-Rme6g8yKlnt0R41e38Nq4WFbndsLG-jR5LcKMJzLmLxTVc8lYtcex6l5RUaV_znqRpZJpZX_VSDaDUFGLFddam4xDRX78PpHW3ri5o5I3hC4YULtXzWFWABKIrL6PQqA4_5w1zWVY497tIYPs3eIDC8kCEWHbJQaa6vKUxE1cfy6m1wcto9oUZWNTMLOUIvqSWBgbax4u_vIOPXS8uQv2ziqhmpe9qFox9OqlWvFe2JOY_GLYbyr13M6RGMEYzAuYOP5QyM36sec3MShvf1Psz-g'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .env\n",
    "config = dotenv.dotenv_values('.env')\n",
    "\n",
    "PASSWORD = config['PASSWORD']\n",
    "\n",
    "CLIENT_SECRET = config['CLIENT_SECRET']\n",
    "\n",
    "USERNAME_EMAIL = config['USERNAME_EMAIL']\n",
    "\n",
    "# Example usage:\n",
    "ACCESS_TOKEN = request_access_token(USERNAME_EMAIL, PASSWORD, CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions\n",
    "\n",
    "def fetch_station_data(station_id, from_date, to_date, BASE_URL, ACCESS_TOKEN):\n",
    "    \"\"\"\n",
    "    Retrieves data for a specified bike hire docking station over a given time period.\n",
    "\n",
    "    This function connects to a specified URL to access data regarding the availability of bikes. It\n",
    "    makes an HTTP GET request with authentication and specific query parameters to gather\n",
    "    aggregated data across the specified dates.\n",
    "\n",
    "    :param station_id: Unique identifier for the bike hire docking station.\n",
    "    :type station_id: str\n",
    "    :param from_date: The start date and time from when the data is to be fetched.\n",
    "    :type from_date: datetime\n",
    "    :param to_date: The end date and time until when the data is to be fetched.\n",
    "    :type to_date: datetime\n",
    "    :param BASE_URL: The base URL of the API where the bike data is hosted.\n",
    "    :type BASE_URL: str\n",
    "    :param ACCESS_TOKEN: The access token for authenticating the API request.\n",
    "    :type ACCESS_TOKEN: str\n",
    "\n",
    "    :return: A JSON object containing the response data if successful, None otherwise. The JSON includes\n",
    "             aggregated available bike numbers per hour. If the request fails, an error message and status\n",
    "             code are printed.\n",
    "    :rtype: dict or None\n",
    "    \n",
    "    :raises Exception: Raises an error with status code and text if the response is unsuccessful.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{BASE_URL}{station_id}\"\n",
    "    headers = {\n",
    "        'NGSILD-Tenant': 'infoportal',\n",
    "        'Authorization': f'Bearer {ACCESS_TOKEN}'\n",
    "    }\n",
    "    params = {\n",
    "        'type': 'BikeHireDockingStation',\n",
    "        'fromDate': from_date.isoformat(),\n",
    "        'toDate': to_date.isoformat(),\n",
    "        'attrs': 'availableBikeNumber',\n",
    "        'aggrPeriod': 'hour',\n",
    "        'aggrMethod': 'avg'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f'Got a response for station_id: {station_id}')\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def create_dataframe_from_api_data(data):\n",
    "    \"\"\"\n",
    "    Converts data received from an API response into a structured pandas DataFrame.\n",
    "\n",
    "    This function parses data from a JSON-like dictionary that includes time indices,\n",
    "    entity identifiers, and various attributes into a DataFrame that can be used for \n",
    "    further data analysis or visualization.\n",
    "\n",
    "    :param data: The data received from an API request, expected to include keys like \n",
    "                 'index', 'entityId', and 'attributes' which contain measurement values.\n",
    "    :type data: dict\n",
    "\n",
    "    :return: A DataFrame with time indices, an entity id extracted from 'entityId', and columns \n",
    "             for each attribute contained within 'attributes'. This DataFrame is restructured to \n",
    "             place 'entityId' and 'time_utc' columns first.\n",
    "    :rtype: pandas.DataFrame\n",
    "\n",
    "    :raises ValueError: If essential keys such as 'index', 'entityId', or 'attributes' are missing in the data.\n",
    "    \"\"\"\n",
    "\n",
    "    if not all(key in data for key in ['index', 'entityId', 'attributes']):\n",
    "        raise ValueError(\"Data missing one of the essential keys: 'index', 'entityId', 'attributes'\")\n",
    "    \n",
    "    # Dictionary to store attribute values\n",
    "    # attribute_data = {}\n",
    "\n",
    "    # Extract the index from the response\n",
    "    time_index = pd.to_datetime(data['index'])\n",
    "\n",
    "    # Extract entityId and entityType\n",
    "    entity_id = data['entityId']\n",
    "\n",
    "    # Extract the number after \"KielRegion\" from the entityId\n",
    "    match = re.search(r'KielRegion:(\\d+)', entity_id)\n",
    "    entity_id_number = match.group(1) if match else ''  # Get the number or set to empty if not found\n",
    "\n",
    "    # Loop through each attribute dictionary in 'attributes'\n",
    "    # for attribute in data['attributes']:\n",
    "    #     attr_name = attribute['attrName']\n",
    "    #     attribute_data[attr_name] = attribute.get('values', [])\n",
    "\n",
    "    # Dictionary to accumulate attribute values\n",
    "    attribute_data = {attr['attrName']: attr.get('values', []) for attr in data['attributes']}\n",
    "    \n",
    "    # Create a pandas DataFrame from the dictionary\n",
    "    df = pd.DataFrame(attribute_data)\n",
    "    # Add the entityId number and index values as new columns\n",
    "    df['entityId'] = entity_id_number\n",
    "    df['time_utc'] = time_index\n",
    "\n",
    "    # Reorder the columns to have 'entityId' first, then 'time', followed by the rest\n",
    "    column_order = ['entityId', 'time_utc'] + [col for col in df.columns if col not in ['entityId', 'time_utc']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def update_and_save_station_data(DATA_FILENAME, STATION_IDS, START_DATE, END_DATE, BASE_URL, ACCESS_TOKEN):\n",
    "    \"\"\"\n",
    "    Updates and saves bike station data by fetching new data for specified stations and dates, then combining it with existing data.\n",
    "\n",
    "    This function first checks if there exists previous data in a specified CSV file,\n",
    "    and loads it if available. It then identifies any gaps in the data between START_DATE and END_DATE,\n",
    "    and makes API requests to fetch missing data for those specific time periods and station IDs.\n",
    "    The newly fetched data is then combined with the previously existing data, the combined data is sorted and saved back to the CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - DATA_FILENAME (str): The file path for reading and writing station data.\n",
    "    - STATION_IDS (list of str): Identifiers for the stations which need data updates.\n",
    "    - START_DATE (datetime): The starting datetime from which data needs to be fetched.\n",
    "    - END_DATE (datetime): The ending datetime until which data needs to be fetched.\n",
    "    - BASE_URL (str): The base URL to which API requests should be made.\n",
    "    - ACCESS_TOKEN (str): The token used for authenticating API requests.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value but prints a message to the console about the process completion or any errors encountered.\n",
    "\n",
    "    No return value is expected. The function logs messages indicating successful or failed data fetching attempts,\n",
    "    and shows the number of new records fetched and total unique stations updated.\n",
    "    If no new data is fetched, it notifies that existing data is used.\n",
    "\n",
    "    Side Effects:\n",
    "    - Read and write operations on a CSV file.\n",
    "    - API requests sent to a remote service.\n",
    "    - Potentially modifies global state if global variables or mutable data types are passed and manipulated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prüfen, ob data_temp.csv vorhanden ist\n",
    "    if os.path.exists(DATA_FILENAME):\n",
    "        # Laden des existierenden DataFrame\n",
    "        old_data_temp = pd.read_csv(DATA_FILENAME)\n",
    "        # make 'time_utc' in datetime\n",
    "        old_data_temp['time_utc'] = pd.to_datetime(old_data_temp['time_utc'])\n",
    "        # lösche alle daten vor START_DATE\n",
    "        old_data_temp = old_data_temp[old_data_temp['time_utc'] >= START_DATE]\n",
    "    else:\n",
    "        # Erstellen eines leeren DataFrame, wenn die Datei nicht existiert\n",
    "        old_data_temp = pd.DataFrame(columns=['entityId', 'time_utc'])\n",
    "\n",
    "    # - timedelta(hours=1), damit der request_start_date nicht gleich END_DATE ist\n",
    "    # full_date_range = all timestamps (until now) of the timewindow needed for the model for prediction\n",
    "    full_date_range = pd.date_range(start=START_DATE, end=END_DATE - timedelta(hours=1), freq='h') \n",
    "    # Liste von DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    for station_id in STATION_IDS:\n",
    "        # überprüfe für station_id, ob der zeitraum von START_DATE bis END_DATE in old_data_temp vorhanden ist:\n",
    "        # select one station\n",
    "        station_data = old_data_temp[old_data_temp['entityId'] == station_id]\n",
    "        # extract available dates\n",
    "        available_dates = station_data['time_utc']\n",
    "        # Ermitteln der fehlenden Daten\n",
    "        missing_dates = full_date_range[~full_date_range.isin(available_dates)]\n",
    "\n",
    "        # wenn ja, skip diese station_id\n",
    "        # wenn nein, mache ein request_start_date\n",
    "\n",
    "        # Daten nur für fehlende Zeiten anfordern\n",
    "        if not missing_dates.empty:\n",
    "            request_start_date = missing_dates[0]\n",
    "            # und requeste die nicht vorhandenen stunden bis zum END_DATE\n",
    "            data = fetch_station_data(station_id, request_start_date, END_DATE, BASE_URL, ACCESS_TOKEN)\n",
    "            if data:\n",
    "                df = create_dataframe_from_api_data(data)\n",
    "                # und appende sie an das dataframe\n",
    "                dataframes.append(df)\n",
    "\n",
    "    if dataframes:\n",
    "        # Alle neuen DataFrames der Stationen zusammenführen\n",
    "        new_data_temp = pd.concat(dataframes)\n",
    "        # make the entitiy_id a number \n",
    "        new_data_temp['entityId'] = new_data_temp['entityId'].astype('int64')\n",
    "        # Zusammenführen des alten DataFrames mit dem neuen\n",
    "        combined_data_temp = pd.concat([old_data_temp, new_data_temp])\n",
    "        # Sortieren, nach entitiyId und time_utc\n",
    "        combined_data_temp = combined_data_temp.sort_values(by=['entityId', 'time_utc'])\n",
    "        # resete index\n",
    "        combined_data_temp = combined_data_temp.reset_index(drop=True)\n",
    "        # DataFrame in eine CSV-Datei speichern\n",
    "        combined_data_temp.to_csv(DATA_FILENAME, index=False)\n",
    "\n",
    "        # count new records and unique Ids \n",
    "        total_new_records = len(new_data_temp)\n",
    "        unique_stations = new_data_temp['entityId'].nunique()\n",
    "\n",
    "        print(f'{total_new_records} new records fetched for {unique_stations} stations.')\n",
    "        print(f'Data successfully loaded and saved for STATION_IDS:{STATION_IDS}')\n",
    "    else:\n",
    "        print('No new data to process, data for every station is available. Existing data used.')\n",
    "\n",
    "    print('-------------')\n",
    "    print(f'Time in UTC:\\nStart Date: {START_DATE}\\nEnd Date: {END_DATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konfigurations\n",
    "\n",
    "# .env file anpassen, für application mit password, username email for access token\n",
    "# config = dotenv.dotenv_values(\".env\")\n",
    "\n",
    "# ACCESS_TOKEN = config[\"ACCESS_TOKEN\"]\n",
    "\n",
    "BASE_URL = \"https://apis.kielregion.addix.io/ql/v2/entities/urn:ngsi-ld:BikeHireDockingStation:KielRegion:\"\n",
    "\n",
    "STATION_IDS = [24370, 24397, 24367, 24399]  # Beispielliste von Station IDs\n",
    "# change to station ids from file einlesen, um änderungen zu haben\n",
    "# this file should contain also the name and lat lon for plotting etc.\n",
    "\n",
    "# API mit UTC time steps\n",
    "# Calculate the end date by rounding down to the closest whole hour in UTC !,\n",
    "# to make sure to get hourly averages for whole hours with API request\n",
    "END_DATE = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "START_DATE = END_DATE - timedelta(days=1) # timedelta anpassen an model sliding window length (=24 hours)\n",
    "\n",
    "DATA_FILENAME = 'data_temp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data to process, data for every station is available. Existing data used.\n",
      "-------------\n",
      "Time in UTC:\n",
      "Start Date: 2024-11-24 17:00:00+00:00\n",
      "End Date: 2024-11-25 17:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "### Usage\n",
    "\n",
    "update_and_save_station_data(DATA_FILENAME, STATION_IDS, START_DATE, END_DATE, BASE_URL, ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
