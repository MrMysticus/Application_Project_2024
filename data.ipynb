{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file for creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements txt !\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import dotenv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer Token successful requested\n",
      "Access Token successfully written to the .env file.\n"
     ]
    }
   ],
   "source": [
    "# .env\n",
    "config = dotenv.dotenv_values(\".env\")\n",
    "\n",
    "PASSWORD = config[\"PASSWORD\"]\n",
    "\n",
    "CLIENT_SECRET = config[\"CLIENT_SECRET\"]\n",
    "\n",
    "token_url = 'https://accounts.kielregion.addix.io/realms/infoportal/protocol/openid-connect/token'\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'justus.heilingbrunner@student.fh-kiel.de', \n",
    "    'password': PASSWORD,\n",
    "    'client_id': 'quantumleap',\n",
    "    'client_secret': CLIENT_SECRET\n",
    "}\n",
    "\n",
    "response = requests.post(token_url, headers=headers, data=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    token_data = response.json()\n",
    "    access_token = token_data['access_token']\n",
    "    print(f\"Bearer Token successful requested\")\n",
    "    if access_token:\n",
    "        dotenv_path = '.env'\n",
    "        \n",
    "        dotenv.set_key(dotenv_path, 'ACCESS_TOKEN', access_token)\n",
    "        print(f\"Access Token successfully written to the .env file.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions\n",
    "\n",
    "def fetch_station_data(station_id, from_date, to_date):\n",
    "    \"\"\"\n",
    "    Retrieves data for a specified bike hire docking station over a given time period.\n",
    "\n",
    "    This function connects to a specified URL to access data regarding the availability of bikes. It\n",
    "    makes an HTTP GET request with authentication and specific query parameters to gather\n",
    "    aggregated data across the specified dates.\n",
    "\n",
    "    :param station_id: Unique identifier for the bike hire docking station.\n",
    "    :type station_id: str\n",
    "\n",
    "    :param from_date: The start date and time from when the data is to be fetched.\n",
    "    :type from_date: datetime\n",
    "\n",
    "    :param to_date: The end date and time until when the data is to be fetched.\n",
    "    :type to_date: datetime\n",
    "\n",
    "    :return: A JSON object containing the response data if successful, None otherwise. The JSON includes\n",
    "             aggregated available bike numbers per hour. If the request fails, an error message and status\n",
    "             code are printed.\n",
    "    :rtype: dict or None\n",
    "    \n",
    "    :raises Exception: Raises a printed error with status code and text if the response is unsuccessful.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{BASE_URL}{station_id}\"\n",
    "    headers = {\n",
    "        'NGSILD-Tenant': 'infoportal',\n",
    "        'Authorization': f'Bearer {ACCESS_TOKEN}'\n",
    "    }\n",
    "    params = {\n",
    "        'type': 'BikeHireDockingStation',\n",
    "        'fromDate': from_date.isoformat(),\n",
    "        'toDate': to_date.isoformat(),\n",
    "        'attrs': 'availableBikeNumber',\n",
    "        'aggrPeriod': 'hour',\n",
    "        'aggrMethod': 'avg'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f'Got a response for station_id: {station_id}')\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def create_dataframe_from_api_data(data):\n",
    "    \"\"\"\n",
    "    Converts data received from an API response into a structured pandas DataFrame.\n",
    "\n",
    "    This function parses data from a JSON-like dictionary that includes time indices,\n",
    "    entity identifiers, and various attributes into a DataFrame that can be used for \n",
    "    further data analysis or visualization.\n",
    "\n",
    "    :param data: The data received from an API request, expected to include keys like \n",
    "                 'index', 'entityId', and 'attributes' which contain measurement values.\n",
    "    :type data: dict\n",
    "\n",
    "    :return: A DataFrame with time indices, an entity id extracted from 'entityId', and columns \n",
    "             for each attribute contained within 'attributes'. This DataFrame is restructured to \n",
    "             place 'entityId' and 'time_utc' columns first.\n",
    "    :rtype: pandas.DataFrame\n",
    "\n",
    "    :raises ValueError: If essential keys such as 'index', 'entityId', or 'attributes' are missing in the data.\n",
    "    \"\"\"\n",
    "\n",
    "    if not all(key in data for key in ['index', 'entityId', 'attributes']):\n",
    "        raise ValueError(\"Data missing one of the essential keys: 'index', 'entityId', 'attributes'\")\n",
    "    \n",
    "    # Dictionary to store attribute values\n",
    "    # attribute_data = {}\n",
    "\n",
    "    # Extract the index from the response\n",
    "    time_index = pd.to_datetime(data['index'])\n",
    "\n",
    "    # Extract entityId and entityType\n",
    "    entity_id = data['entityId']\n",
    "\n",
    "    # Extract the number after \"KielRegion\" from the entityId\n",
    "    match = re.search(r'KielRegion:(\\d+)', entity_id)\n",
    "    entity_id_number = match.group(1) if match else ''  # Get the number or set to empty if not found\n",
    "\n",
    "    # Loop through each attribute dictionary in 'attributes'\n",
    "    # for attribute in data['attributes']:\n",
    "    #     attr_name = attribute['attrName']\n",
    "    #     attribute_data[attr_name] = attribute.get('values', [])\n",
    "\n",
    "    # Dictionary to accumulate attribute values\n",
    "    attribute_data = {attr['attrName']: attr.get('values', []) for attr in data['attributes']}\n",
    "    \n",
    "    # Create a pandas DataFrame from the dictionary\n",
    "    df = pd.DataFrame(attribute_data)\n",
    "    # Add the entityId number and index values as new columns\n",
    "    df['entityId'] = entity_id_number\n",
    "    df['time_utc'] = time_index\n",
    "\n",
    "    # Reorder the columns to have 'entityId' first, then 'time', followed by the rest\n",
    "    column_order = ['entityId', 'time_utc'] + [col for col in df.columns if col not in ['entityId', 'time_utc']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def update_and_save_station_data(DATA_TEMP_FILENAME, STATION_IDS, START_DATE, END_DATE):\n",
    "    \"\"\"\n",
    "    Updates and saves bike station data for a specified period.\n",
    "    \n",
    "    This function checks for existing data from a CSV file, updates it with new data from a specified \n",
    "    time range if data for some dates are missing, and then saves the updated data back to the CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - DATA_TEMP_FILENAME (str): The path to the data file to read from and write to.\n",
    "    - STATION_IDS (list): A list of station identifiers for which data needs to be updated.\n",
    "    - START_DATE (datetime): The start datetime from which to fetch new data.\n",
    "    - END_DATE (datetime): The end datetime until which to fetch new data.\n",
    "\n",
    "    The function fetches data using an API call for each station ID, checks for dates where data is missing,\n",
    "    and requests data for those dates. It then combines this with existing data, sorts it, and saves\n",
    "    it back to a CSV file.\n",
    "\n",
    "    Prints messages to indicate successful data processing or if no new data were processed.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Prüfen, ob data_temp.csv vorhanden ist\n",
    "    if os.path.exists(DATA_TEMP_FILENAME):\n",
    "        # Laden des existierenden DataFrame\n",
    "        old_data_temp = pd.read_csv(DATA_TEMP_FILENAME)\n",
    "        # make 'time_utc' in datetime\n",
    "        old_data_temp['time_utc'] = pd.to_datetime(old_data_temp['time_utc'])\n",
    "        # lösche alle daten vor START_DATE\n",
    "        old_data_temp = old_data_temp[old_data_temp['time_utc'] >= START_DATE]\n",
    "    else:\n",
    "        # Erstellen eines leeren DataFrame, wenn die Datei nicht existiert\n",
    "        old_data_temp = pd.DataFrame(columns=['entityId', 'time_utc'])\n",
    "\n",
    "    #  - timedelta(hours=1), damit der request_start_date nicht gleich END_DATE ist\n",
    "    full_date_range = pd.date_range(start=START_DATE, end=END_DATE - timedelta(hours=1), freq='h') \n",
    "    # Liste von DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    for station_id in STATION_IDS:\n",
    "        # überprüfe für station_id, ob der zeitraum von START_DATE bis END_DATE in old_data_temp vorhanden ist:\n",
    "        # select one station\n",
    "        station_data = old_data_temp[old_data_temp['entityId'] == station_id]\n",
    "        # extract available dates\n",
    "        available_dates = station_data['time_utc']\n",
    "        # Ermitteln der fehlenden Daten\n",
    "        missing_dates = full_date_range[~full_date_range.isin(available_dates)]\n",
    "\n",
    "        # wenn ja, skip diese station_id\n",
    "        # wenn nein, mache ein request_start_date\n",
    "\n",
    "        # Daten nur für fehlende Zeiten anfordern\n",
    "        if not missing_dates.empty:\n",
    "            request_start_date = missing_dates[0]\n",
    "            # und requeste die nicht vorhandenen stunden bis zum END_DATE\n",
    "            data = fetch_station_data(station_id, request_start_date, END_DATE)\n",
    "            if data:\n",
    "                df = create_dataframe_from_api_data(data)\n",
    "                # und appende sie an das dataframe\n",
    "                dataframes.append(df)\n",
    "\n",
    "    if dataframes:\n",
    "        # Alle neuen DataFrames der Stationen zusammenführen\n",
    "        new_data_temp = pd.concat(dataframes)\n",
    "        # make the entitiy_id a number \n",
    "        new_data_temp['entityId'] = new_data_temp['entityId'].astype('int64')\n",
    "        # Zusammenführen des alten DataFrames mit dem neuen\n",
    "        combined_data_temp = pd.concat([old_data_temp, new_data_temp])\n",
    "        # Sortieren, nach entitiyId und time_utc\n",
    "        combined_data_temp = combined_data_temp.sort_values(by=['entityId', 'time_utc'])\n",
    "        # resete index\n",
    "        combined_data_temp = combined_data_temp.reset_index(drop=True)\n",
    "        # DataFrame in eine CSV-Datei speichern\n",
    "        combined_data_temp.to_csv(DATA_TEMP_FILENAME, index=False)\n",
    "\n",
    "        # count new records and unique Ids \n",
    "        total_new_records = len(new_data_temp)\n",
    "        unique_stations = new_data_temp['entityId'].nunique()\n",
    "\n",
    "        print(f'{total_new_records} new records fetched for {unique_stations} stations.')\n",
    "    else:\n",
    "        print('No new data to process. Existing data used.')\n",
    "\n",
    "    print('-------------')\n",
    "    print(f'Data successfully loaded and saved for STATION_IDS:{STATION_IDS}')\n",
    "    print(f'Time in UTC:\\nStart Date: {START_DATE}\\nEnd Date: {END_DATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konfigurations\n",
    "\n",
    "config = dotenv.dotenv_values(\".env\")\n",
    "\n",
    "ACCESS_TOKEN = config[\"ACCESS_TOKEN\"]\n",
    "\n",
    "BASE_URL = \"https://apis.kielregion.addix.io/ql/v2/entities/urn:ngsi-ld:BikeHireDockingStation:KielRegion:\"\n",
    "\n",
    "STATION_IDS = [24370, 24397, 24367, 24399]  # Beispielliste von Station IDs\n",
    "\n",
    "# API mit UTC time steps\n",
    "# Calculate the end date by rounding down to the closest whole hour in UTC !,\n",
    "# to make sure to get hourly averages for whole hours with API request\n",
    "END_DATE = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)\n",
    "START_DATE = END_DATE - timedelta(days=1) # timedelta anpassen an model sliding window length (=24 hours)\n",
    "\n",
    "DATA_TEMP_FILENAME = 'data_temp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a response for station_id: 24399\n",
      "24 new records fetched for 1 stations.\n",
      "-------------\n",
      "Data successfully loaded and saved for STATION_IDS:[24370, 24397, 24367, 24399]\n",
      "Time in UTC:\n",
      "Start Date: 2024-11-24 07:00:00+00:00\n",
      "End Date: 2024-11-25 07:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "### Usage\n",
    "\n",
    "update_and_save_station_data(DATA_TEMP_FILENAME, STATION_IDS, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
